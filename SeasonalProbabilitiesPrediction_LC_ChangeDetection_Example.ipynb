{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the process for analyzing the ability to predict whether change from $year_{i}$ to $year_{i+1}$ is consistent using the seasonal land cover probabilities (sometimes referred to as the Brookie-Brumby method). \n",
    "The definition of consistent change is up to the user, but one example of consistent change is if from $year_{i}$ to $year_{i+1}$ there was change, and from $year_{i+1}$ to $year_{i+2}$ the state stays the same.\n",
    "\n",
    "    \n",
    "To run this notebook, you will need:\n",
    "1. An image collection of land cover classifications for each time period (e.g. year). The classification images should be images with one band representing the classificaton for that time period.\n",
    "    * You can squash scene by scene predictions to annual\n",
    "    * Or have the final classifications for each year already processed\n",
    "2. An image collection of land cover probabilities with seasonal or sub-seasonal temporal resolution (e.g., scene-by-scene, monthly, or seasonal classifications)\n",
    "2. An area of interest over which to predict land cover change (e.g. global versus one country like Brazil)\n",
    "\n",
    "\n",
    "    \n",
    "To more formally describe the Brookie-Brumby method: \n",
    "* The model is predicting whether change from $year_{i}$ to $year_{i+1}$ is consistent, given seasonal land cover probabilities for various years, such as $year_{i}$, $year_{i+1}$, and other years for which data is available.\n",
    "* The model is a binary classification model\n",
    "* The inputs are seasonal land cover probabilities of the pixel for $year_{i}$ and the difference in probabilities from $year_{i}$ to $year_{i+1}$ (other years can also be included). Seasons can be defined by the user (such as four seasons in temperate regions, wet and dry seasons in tropical regions).\n",
    "* The output of the model is a probability ranging from 0-1 that the transition is consistent\n",
    "\n",
    "The steps of this notebook are as follows:\n",
    "1. Define consistent change\n",
    "2. Define predictor variables \n",
    "3. Gather a training set, validation set, and test set\n",
    "    * For simplicity we will only look at one year of change.\n",
    "4. Define models we will test (e.g logistic regression, random forest, maxEntropy)\n",
    "5. Use cross validation to choose optimal parameters for each model\n",
    "6. Train a model with the optimal parameters for each model and compare on validation set to choose the best model.\n",
    "7. Use model to predict consistent change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and iniatilize Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.8/site-packages')\n",
    "sys.path.append('/Users/kristine/Library/Python/3.8/lib/python/site-packages')\n",
    "sys.path.append('/usr/local/lib/python3.8/site-packages/mtlchmm-0.0.2-py3.8.egg')\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Map, basemaps\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# relative import for this folder hierarchy, credit: https://stackoverflow.com/a/35273613\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "\n",
    "from wri_change_detection import preprocessing as npv\n",
    "from wri_change_detection import gee_classifier as gclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Iniatilize Earth Engine and Google Cloud authentication</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize earth engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define a seed number to ensure reproducibility across random processes. This seed will be used in all subsequent sampling as well. We'll also define seeds for sampling the training, validation, and test sets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seed=30\n",
    "random.seed(num_seed)\n",
    "\n",
    "missing_value = -32768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Land Cover Classifications and Define Consistent Change\n",
    "\n",
    "\n",
    "<font size=\"4\">We will start by gathering monthly land cover probabilities and squashing them to annual classifications by taking the median.\n",
    "Define land cover classification image collection, with one image for each time period. Each image should have one band representing the classification in that pixel for one time period.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load collection\n",
    "#This collection represents monthly dynamic world classifications of land cover, later we'll squash it to annual\n",
    "dynamic_world_classifications_monthly = ee.ImageCollection('projects/wings-203121/assets/dynamic-world/v3-5_stack_tests/wri_test_goldsboro')\n",
    "\n",
    "#Get classes from first image\n",
    "dw_classes = dynamic_world_classifications_monthly.first().bandNames()\n",
    "dw_classes_str = dw_classes.getInfo()\n",
    "\n",
    "#Get dictionary of classes and values\n",
    "#Define array of land cover classification values\n",
    "dw_class_values = np.arange(1,10).tolist()\n",
    "dw_class_values = ee.List(dw_class_values)\n",
    "#Create dictionary representing land cover classes and land cover class values\n",
    "dw_classes_dict = ee.Dictionary.fromLists(dw_classes, dw_class_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define color palettes to map land cover</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_detection_palette = ['#ffffff', # no_data=0\n",
    "                              '#419bdf', # water=1\n",
    "                              '#397d49', # trees=2\n",
    "                              '#88b053', # grass=3\n",
    "                              '#7a87c6', # flooded_vegetation=4\n",
    "                              '#e49535', # crops=5\n",
    "                              '#dfc25a', # scrub_shrub=6\n",
    "                              '#c4291b', # builtup=7\n",
    "                              '#a59b8f', # bare_ground=8\n",
    "                              '#a8ebff', # snow_ice=9\n",
    "                              '#616161', # clouds=10\n",
    "]\n",
    "statesViz = {'min': 0, 'max': 10, 'palette': change_detection_palette};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Gather projection and geometry information from the land cover classifications</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = dynamic_world_classifications_monthly.first().projection().getInfo()\n",
    "crs = projection.get('crs')\n",
    "crsTransform = projection.get('transform')\n",
    "scale = dynamic_world_classifications_monthly.first().projection().nominalScale().getInfo()\n",
    "print('CRS and Transform: ',crs, crsTransform)\n",
    "\n",
    "geometry = dynamic_world_classifications_monthly.first().geometry().bounds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define years to assess land cover and convert monthly classifications to annual classifications</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Define years to get annual classifications for\n",
    "years = np.arange(2016,2020)\n",
    "\n",
    "#Squash scenes from monthly to annual to get the annual classification\n",
    "dynamic_world_classifications = npv.squashScenesToAnnualClassification(dynamic_world_classifications_monthly,years,method='median')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define consistent change image to sample training points. Here we will define consistent change as:\n",
    "* $$\\ state(i) != state(i+1) $$\n",
    "* $$\\ state(i+1) = state(i+2) $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert classifications to an image with band names as original image names\n",
    "dynamic_world_classification_image = dynamic_world_classifications.toBands()\n",
    "\n",
    "#Get a collection of images where each image has 3 bands: classifications for year(i), classifications for year(i+1), and classifications for year i+2\n",
    "lc_consistent_change_one_year_col = npv.getYearStackIC(dynamic_world_classification_image,dynamic_world_classification_image.bandNames().getInfo(), band_indices=[0,1,2])\n",
    "\n",
    "#Get a collection of images where each image represents whether change was consistent from year(i) to year(i+1)\n",
    "lc_consistent_change_one_year_col = lc_consistent_change_one_year_col.map(npv.LC_ConsistentChangeOneYear)\n",
    "\n",
    "#Convert this collection to one image\n",
    "lc_consistent_change_one_year_image = lc_consistent_change_one_year_col.toBands()\n",
    "\n",
    "#Rename image bands to the central year\n",
    "lc_consistent_change_one_year_image = lc_consistent_change_one_year_image.rename(lc_consistent_change_one_year_col.aggregate_array('OriginalBand'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Now *lc_consistent_change_one_year_image* is a binary multi-band image where each band represents\n",
    "whether change from $year_{i}$ to $year_{i+1}$ is consistent\n",
    "\n",
    "However it also includes 0's for where there was no change from $year_{i}$ to $year_{i+1}$, therefore we need to\n",
    "mask the *lc_consistent_change_one_year_image* to only pixels with change form $year_{i}$ to $year_{i+1}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Mask consistent change image to pixels with change in that year</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a collection of images where each image has 2 bands: classifications for year(i) and classifications for year(i+1)\n",
    "lc_one_change_col = npv.getYearStackIC(dynamic_world_classification_image,dynamic_world_classification_image.bandNames().getInfo(), band_indices=[0,1])\n",
    "\n",
    "#Get a collection of images where each image represents whether there was change from year(i) to year(i+1)\n",
    "lc_one_change_col = lc_one_change_col.map(npv.LC_OneChange)\n",
    "\n",
    "#Convert this collection to one image\n",
    "lc_one_change_image = lc_one_change_col.toBands()\n",
    "\n",
    "#Rename image bands to the central year\n",
    "lc_one_change_image = lc_one_change_image.rename(lc_one_change_col.aggregate_array('OriginalBand'))\n",
    "\n",
    "#Mask each band of the consistent change image to \n",
    "lc_consistent_change_one_year_image = lc_consistent_change_one_year_image.mask(lc_one_change_image.select(lc_consistent_change_one_year_image.bandNames()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Map three years worth of land cover and the consistent change image to check *lc_consistent_change_one_year_image*\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneChangeDetectionViz = {'min': 0, 'max': 1, 'palette': ['696a76','ff2b2b']}; #gray = 0, red = 1\n",
    "consistentChangeDetectionViz = {'min': 0, 'max': 1, 'palette': ['0741df','df07b5']}; #blue = 0, pink = 1\n",
    "\n",
    "\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map1 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map1.addLayer(dynamic_world_classification_image.select('2017_class'),statesViz,name='2017 LC')\n",
    "Map1.addLayer(dynamic_world_classification_image.select('2018_class'),statesViz,name='2018 LC')\n",
    "Map1.addLayer(dynamic_world_classification_image.select('2019_class'),statesViz,name='2019 LC')\n",
    "Map1.addLayer(lc_one_change_image.select('2017_class'),oneChangeDetectionViz,name='One Change from 2017-2018')\n",
    "Map1.addLayer(lc_consistent_change_one_year_image.select('2017_class'),consistentChangeDetectionViz,\n",
    "              name='Consistent Change from 2017-2019')\n",
    "display(Map1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first layer will show classifications for 2017\n",
    "* The second layer will show classifications for 2018\n",
    "* The third layer will show classifications for 2019\n",
    "* The fourth layer will show whether change occured from 2017 to 2018, with grey for no and red for yes\n",
    "* The fifth layer will show whether change from 2017 to 2018 stayed consistent in 2019, with blue for no and pink for yes\n",
    "* The final layer should be masked to show only consistent change in the red areas of the fourth layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Gather neighborhood predictor variables \n",
    "<font size=\"4\"> Here we will be collecting predictor variables on seasonal probabilities and the difference in seasonal probabilities from $year_{i}$ to $year_{i+1}$. \n",
    "    \n",
    "For the remainder of the notebook, we'll sample training points and predictor variables for one year only.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate seasonal probabilities and the differece in seasonal probabilities from the current year to the following year\n",
    "#using the getSeasonalDifference function.\n",
    "\n",
    "#Select year to calculate probabilities and probability differences\n",
    "year = 2017\n",
    "\n",
    "#Get band names\n",
    "dwc_bandNames = dynamic_world_classifications_monthly.first().bandNames().getInfo()\n",
    "\n",
    "#Caclulate seasonal probabilities and probability differences\n",
    "#Inputs:\n",
    "#    probability_collection is the monthly classifications\n",
    "#    year = 2017\n",
    "#    band_names = dwc_bandNames, gatehred from dynamic_world_classifications_monthly\n",
    "#    reduce_method = 'median', where seasonal probabilities are calculated as the median of the monthly probabilities\n",
    "#    seson_list is used to define the seasons and give them names, including the year, month, and day of the start of the season\n",
    "#          and the year, month, and day of the end of the season\n",
    "#    include_difference is set to True to include the difference in seasonal probabilities from year(i) to year(i+year_difference)\n",
    "#    year_difference is set to 1 to calculate the seasonal difference from year(i) to year(i+1)\n",
    "#    image_name is used to name the output image using year\n",
    "\n",
    "#Outputs:\n",
    "#    an image of seasonal probabilities for each land cover class and the difference in those probabilities\n",
    "#    from year(i) to year(i+1)\n",
    "\n",
    "season_changes = npv.getSeasonalDifference(probability_collection = dynamic_world_classifications_monthly, \n",
    "                                           year = year, \n",
    "                                           band_names = dwc_bandNames, \n",
    "                                           reduce_method='median', \n",
    "                                           season_list = [['winter',-1,12,1,0,2,'end'],\n",
    "                                                          ['spring',0,3,1,0,5,'end'],\n",
    "                                                          ['summer',0,6,1,0,8,'end'],\n",
    "                                                          ['fall',0,9,1,0,11,'end']],\n",
    "                                           include_difference=True,\n",
    "                                           year_difference=1,\n",
    "                                           image_name = 'season_probs_{}')\n",
    "\n",
    "#Print the name to see if there are any errors\n",
    "seasonal_bands = season_changes.bandNames().getInfo()\n",
    "seasonal_image_name = season_changes.get('system:index').getInfo()\n",
    "print('Image name: ', seasonal_image_name)\n",
    "print('Bands: ', seasonal_bands)\n",
    "print('Number of bands: ', len(seasonal_bands))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "Map predictor variables to view any areas of missing data. Looking at the 'Summer Trees' (the probability of trees in the summer) and \"Summer Diff Trees' (the difference in probability from 2017-2018 in trees) layers we can see a lot of missing data. These areas will not be sampled from or predicted over using the EE Classifiers.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenViz = {'min': 0, 'max': 1, 'palette': ['ffffff','16ba21']}; #white = 0, green = 1\n",
    "blueViz = {'min': 0, 'max': 1, 'palette': ['ffffff','3360ff']}; #white = 0, blue = 1\n",
    "redViz = {'min': 0, 'max': 1, 'palette': ['ffffff','ff3333']}; #white = 0, blue = 1\n",
    "\n",
    "\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map2 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map2.addLayer(dynamic_world_classification_image.select('2017_class'),statesViz,name='2017 LC')\n",
    "Map2.addLayer(season_changes.select('winter_start_trees'),greenViz,name='Winter Trees')\n",
    "Map2.addLayer(season_changes.select('spring_start_trees'),blueViz,name='Spring Trees')\n",
    "Map2.addLayer(season_changes.select('summer_start_trees'),redViz,name='Summer Trees')\n",
    "Map2.addLayer(season_changes.select('fall_start_trees'),greenViz,name='Fall Trees')\n",
    "Map2.addLayer(season_changes.select('winter_difference_trees'),greenViz,name='Winter Diff Trees')\n",
    "Map2.addLayer(season_changes.select('spring_difference_trees'),blueViz,name='Spring Diff Trees')\n",
    "Map2.addLayer(season_changes.select('summer_difference_trees'),redViz,name='Summer Diff Trees')\n",
    "Map2.addLayer(season_changes.select('fall_difference_trees'),greenViz,name='Fall Diff Trees')\n",
    "display(Map2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Now we have mutliple predictor variables including:\n",
    "1. Land cover probabilities for spring, summer, fall and winter in 2017\n",
    "2. The difference in land cover probabilities from 2017 to 2018 for spring, summer, fall, and winter\n",
    "\n",
    "More predictor variables could be included by calculating the difference in land cover probabilities from 2016 to 2017 or the difference in land cover probabilities form 2016 to 2018.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define predictor variable image and band names to use in model\n",
    "#Here we're just using the seasonal probabilities and differences from the cell above\n",
    "predictor_variable_image = season_changes\n",
    "predictor_variable_names = seasonal_bands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Gather Training Points\n",
    "\n",
    "<font size=\"4\">Now that we have a binary image of where consistent change occurs (masked to areas of change) and our predictor variables, we'll build our training, validation, and test set. We'll only sample training points from one year to simplify the process, however you can sample locations from multiple years.\n",
    "\n",
    "First we'll sample the training point locations by taking stratified samples of the consistent change image, then we'll sample the predictor variables at those locations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Define year for which to sample consistent change, here we'll select 2017\n",
    "year_to_select = '2017_class'\n",
    "\n",
    "#We'll remove training points that are within a certain distance to the validation and test points\n",
    "#However since we're testing over a small area, we'll keep the distance small\n",
    "distanceToFilter = 100\n",
    "\n",
    "\n",
    "#Select consistent change for 2017\n",
    "lc_consistent_change_one_year_select = lc_consistent_change_one_year_image.select(year_to_select).rename('Consistent Change')\n",
    "\n",
    "#Sample point locations that we will split into training, validation, and test sets\n",
    "point_locations = npv.getStratifiedSampleBandPoints(lc_consistent_change_one_year_select, region=geometry, \n",
    "                                                       numPoints=2000, bandName='Consistent Change',seed=num_seed,\n",
    "                                                       geometries=True,scale=scale, projection=crs)\n",
    "\n",
    "#Assign random value between 0 and 1 to split into training, validation, and test sets\n",
    "point_locations = point_locations.randomColumn(columnName='TrainingSplit', seed=num_seed)\n",
    "#First split training set from the rest, taking 70% of the points for training\n",
    "#Roughly 70% training, 30% for validation + testing\n",
    "training_split = 0.7\n",
    "training_locations = point_locations.filter(ee.Filter.lt('TrainingSplit', training_split))\n",
    "validation_and_test = point_locations.filter(ee.Filter.gte('TrainingSplit', training_split))\n",
    "\n",
    "#Define distance filter to remove training points within a certain distance of test points and validation points\n",
    "distFilter = ee.Filter.withinDistance(distance=distanceToFilter, leftField='.geo', rightField= '.geo', maxError= 1)\n",
    "join = ee.Join.inverted()\n",
    "training_locations = join.apply(training_locations, validation_and_test, distFilter);\n",
    "\n",
    "#Assign another random value between 0 and 1 to validation_and_test to split to validation and test sets\n",
    "validation_and_test = validation_and_test.randomColumn(columnName='ValidationSplit', seed=num_seed)\n",
    "#Of the 30% saved for validation + testing, half goes to validation and half goes to test\n",
    "#Meaning original sample will be 70% training, 15% validation, 15% testing\n",
    "validation_split = 0.5 \n",
    "validation_locations = validation_and_test.filter(ee.Filter.lt('ValidationSplit', validation_split))\n",
    "test_locations = validation_and_test.filter(ee.Filter.gte('ValidationSplit', validation_split))\n",
    "\n",
    "#Apply distance filter to remove validation points within a certain distance of test points\n",
    "validation_locations = join.apply(validation_locations, test_locations, distFilter);\n",
    "\n",
    "#Export these locations to an Earth Engine asset (we're using the same training point locations as another demo notebook: https://github.com/wri/rw-dynamicworld-cd/blob/master/LandCoverChangeDetection_Example_NeighborhoodPrediction.ipynb)\n",
    "location_description = '{}_locations'\n",
    "location_assetID = 'projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=training_locations, \n",
    "    description = location_description.format('training'), \n",
    "    assetId = location_assetID.format('training'))\n",
    "export_results_task.start()\n",
    "    \n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=validation_locations, \n",
    "    description = location_description.format('validation'), \n",
    "    assetId = location_assetID.format('validation'))\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=test_locations, \n",
    "    description = location_description.format('test'), \n",
    "    assetId = location_assetID.format('test'))\n",
    "export_results_task.start()\n",
    " \n",
    "#Wait for last export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Next we'll sample the predictor variables for the training, validation, and test sets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define properties to export to an Earth Engine asset\n",
    "points_description = 'Probability_2017-2018_{}_points'\n",
    "points_assetID = 'projects/wri-datalab/DynamicWorld_CD/ModelResults/Probability_2017-2018_{}_points'\n",
    "\n",
    "#Load the point locations assets\n",
    "training_locations_asset = ee.FeatureCollection('projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'.format('training'))\n",
    "validation_locations_asset = ee.FeatureCollection('projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'.format('validation'))\n",
    "test_locations_asset = ee.FeatureCollection('projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'.format('test'))\n",
    "\n",
    "#Sample the neighborhood image at point locations \n",
    "training_points = predictor_variable_image.sampleRegions(training_locations_asset, scale=scale, projection=crs, geometries=True,tileScale=1)\n",
    "validation_points = predictor_variable_image.sampleRegions(validation_locations_asset, scale=scale, projection=crs, geometries=True,tileScale=1)\n",
    "test_points = predictor_variable_image.sampleRegions(test_locations_asset, scale=scale, projection=crs, geometries=True,tileScale=1)\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=training_points, \n",
    "    description = points_description.format('training'), \n",
    "    assetId = points_assetID.format('training'))\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=validation_points, \n",
    "    description = points_description.format('validation'), \n",
    "    assetId = points_assetID.format('validation'))\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=test_points, \n",
    "    description = points_description.format('test'), \n",
    "    assetId = points_assetID.format('test'))\n",
    "export_results_task.start()\n",
    "\n",
    "#Wait for export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: define models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Define dictionaries of parameters and models to test\n",
    "#You can find the inputs for the parameters under the ee.Classifiers section of GEE\n",
    "\n",
    "rf_parameters = {'seed':[num_seed], \n",
    "          'numberOfTrees': [50,100], \n",
    "          'variablesPerSplit': [4,8,10,None], \n",
    "          'minLeafPopulation': [None,10,50], \n",
    "          'bagFraction': [None,0.5,.3], \n",
    "          'maxNodes': [None, 20, 50]\n",
    "         }\n",
    "#buildGridSearchList converts the parameter dictionary into a list of classifiers that can be used in cross-validation\n",
    "rf_classifier_list = gclass.buildGridSearchList(rf_parameters,'smileRandomForest')\n",
    "\n",
    "svm_parameters = {'decisionProcedure':[None]}\n",
    "svm_classifier_list = gclass.buildGridSearchList(svm_parameters,'libsvm')\n",
    "\n",
    "maxent_parameters = {'minIterations':[10,100],\n",
    "                    'maxIterations':[50,200]}\n",
    "maxent_classifier_list = gclass.buildGridSearchList(maxent_parameters,'gmoMaxEnt')\n",
    "\n",
    "classifier_list = rf_classifier_list+svm_classifier_list+maxent_classifier_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Use cross validation to choose optimal parameters for each model, kernel option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name y_column\n",
    "y_column = 'Consistent Change'\n",
    "\n",
    "#Define assetId and description format to export to GEE\n",
    "cv_results_assetId = 'projects/wri-datalab/DynamicWorld_CD/ModelResults/cv_export_probability_2017-2018'\n",
    "cv_results_description = 'cv_export_probability_2017-2018'\n",
    "\n",
    "#Load training points\n",
    "training_points = ee.FeatureCollection(points_assetID.format('training'))\n",
    "\n",
    "#Perform cross validation, returns a feature collection\n",
    "cv_results = gclass.kFoldCrossValidation(inputtedFeatureCollection = training_points, \n",
    "                                     propertyToPredictAsString = y_column, \n",
    "                                     predictors = predictor_variable_names, \n",
    "                                     listOfClassifiers = classifier_list,\n",
    "                                     k=3,seed=num_seed)\n",
    "#Export results to GEE\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=cv_results, \n",
    "        description = cv_results_description, \n",
    "        assetId = cv_results_assetId)\n",
    "export_results_task.start()\n",
    "\n",
    "#Wait for export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Load cross validation results and find the best model based on accuracy on the validation set</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty dataframe to save results of cross validation\n",
    "accuracy_and_keys = pd.DataFrame()\n",
    "\n",
    "#Load cross-validation results\n",
    "results = ee.FeatureCollection(cv_results_assetId)\n",
    "#Get the best result by the cross validation score\n",
    "best_result = results.sort('Validation Score', False).first()\n",
    "\n",
    "#Load params as a dictionary\n",
    "params = best_result.get('Params').getInfo()\n",
    "params = ast.literal_eval(params)\n",
    "\n",
    "#Get the calssifier name\n",
    "classifierName = best_result.get('Classifier Type').getInfo()\n",
    "\n",
    "#Load classifier with best params\n",
    "best_model = gclass.defineClassifier(params,classifierName)\n",
    "\n",
    "#Load training and validation points\n",
    "training_points = ee.FeatureCollection(points_assetID.format('training'))\n",
    "validation_points = ee.FeatureCollection(points_assetID.format('validation'))\n",
    "\n",
    "#Train a classifier with the best params on the training data\n",
    "best_model = best_model.train(training_points, classProperty=y_column, \n",
    "                              inputProperties=predictor_variable_names, subsamplingSeed=num_seed)\n",
    "\n",
    "#Predict over validation data\n",
    "validation_points_predicted = validation_points.classify(best_model)\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "print('--------------------------------------------------------------')\n",
    "print('Final Model Validation Set Confusion Matrix')\n",
    "print(gclass.pretty_print_confusion_matrix(confusion_matrix.getInfo()))\n",
    "print('Final Model Validation Set Accuracy',accuracy)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#Predict over test data\n",
    "test_points = ee.FeatureCollection(points_assetID.format('test'))\n",
    "\n",
    "test_points_predicted = test_points.classify(best_model)\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = test_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "print('--------------------------------------------------------------')\n",
    "print('Final Model Test Set Confusion Matrix')\n",
    "print(gclass.pretty_print_confusion_matrix(confusion_matrix.getInfo()))\n",
    "print('Final Model Test Set Accuracy',accuracy)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Use model to predict consistent change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use best model and predictor variable image to predict consistent change\n",
    "predicted_consistent_change = predictor_variable_image.updateMask(lc_one_change_image.select(year_to_select)).classify(best_model)\n",
    "\n",
    "#The actual consistent change was saved to a variable lc_consistent_change_one_year_select earlier\n",
    "\n",
    "#Map the results\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map3 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map3.addLayer(dynamic_world_classification_image.select('2017_class'),statesViz,name='2017 LC')\n",
    "Map3.addLayer(dynamic_world_classification_image.select('2018_class'),statesViz,name='2018 LC')\n",
    "Map3.addLayer(dynamic_world_classification_image.select('2019_class'),statesViz,name='2019 LC')\n",
    "Map3.addLayer(lc_consistent_change_one_year_select,consistentChangeDetectionViz,\n",
    "              name='Consistent Change from 2017-2019')\n",
    "Map3.addLayer(predicted_consistent_change,oneChangeDetectionViz,\n",
    "              name='Predicted Consistent Change from 2017-2018')\n",
    "\n",
    "display(Map3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first layer will show classifications for 2017\n",
    "* The second layer will show classifications for 2018\n",
    "* The third layer will show classifications for 2019\n",
    "* The fourth layer will show whether change from 2017 to 2018 stayed consistent in 2019, with grey for no and red for yes with blue for no and pink for yes\n",
    "* The fifth layer will show the model's prediction on whether change from 2017 to 2018 stayed consistent in 2019, blue for no and pink for yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can view/save the model using the \"explain\" function of classifiers\n",
    "classifierString = best_model.explain().get('trees')\n",
    "print(classifierString.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
