{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the process for analyzing the ability to predict whether change from $year_{i}$ to $year_{i+1}$ is consistent using the properties of neighboring pixels (sometimes referred to as the Francis Method). \n",
    "The definition of consistent change is up to the user, but one example of consistent change is if from $year_{i}$ to $year_{i+1}$ there was change, and from $year_{i+1}$ to $year_{i+2}$ the state stays the same.\n",
    "\n",
    "    \n",
    "To run this notebook, you will need:\n",
    "1. An image collection of land cover classifications for each time period (e.g. year). The classification images should be images with one band representing the classificaton for that time period.\n",
    "    * You can squash scene by scene predictions to annual\n",
    "    * Or have the final classifications for each year already processed\n",
    "2. An area of interest over which to predict land cover change (e.g. global versus one country like Brazil)\n",
    "\n",
    "\n",
    "    \n",
    "To more formally describe the Francis method: \n",
    "* The model is predicting whether change from $year_{i}$ to $year_{i+1}$ is consistent, given properties of the neighboring pixels \n",
    "* The model is a binary classification model\n",
    "* The inputs are properties of the surrounding neighborhood, for example how many of the surrounding pixels transitioned, how many of the surroudning pixels are of each class, etc\n",
    "* Neighborhoods are defined using kernels, and there are many options for the kernel including shape and size\n",
    "* The output of the model is a probability ranging from 0-1 that the transition is consistent\n",
    "\n",
    "The steps of this notebook are as follows:\n",
    "1. Define consistent change\n",
    "2. Define predictor variables \n",
    "3. Define kernels to test different neighborhoods\n",
    "4. Gather a training set, validation set, and test set\n",
    "    * For simplicity we will only look at one year of change.\n",
    "5. Define models we will test (e.g logistic regression, random forest, maxEntropy)\n",
    "6. Use cross validation to choose optimal parameters for each model, kernel option\n",
    "7. Train a model with the optimal parameters for each model, kernel option and compare on validation set to choose the best model.\n",
    "8. Use model to predict consistent change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and iniatilize Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.8/site-packages')\n",
    "sys.path.append('/Users/kristine/Library/Python/3.8/lib/python/site-packages')\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Map, basemaps\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# relative import for this folder hierarchy, credit: https://stackoverflow.com/a/35273613\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "\n",
    "from wri_change_detection import preprocessing as npv\n",
    "from wri_change_detection import gee_classifier as gclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Iniatilize Earth Engine and Google Cloud authentication</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize earth engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define a seed number to ensure reproducibility across random processes. This seed will be used in all subsequent sampling as well. We'll also define seeds for sampling the training, validation, and test sets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seed=30\n",
    "random.seed(num_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Land Cover Classifications and Define Consistent Change\n",
    "\n",
    "\n",
    "<font size=\"4\">We will start by gathering monthly land cover probabilities and squashing them to annual classifications by taking the median.\n",
    "Define land cover classification image collection, with one image for each time period. Each image should have one band representing the classification in that pixel for one time period.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load collection\n",
    "#This collection represents monthly dynamic world classifications of land cover, later we'll squash it to annual\n",
    "dynamic_world_classifications_monthly = ee.ImageCollection('projects/wings-203121/assets/dynamic-world/v3-5_stack_tests/wri_test_goldsboro')\n",
    "\n",
    "#Get classes from first image\n",
    "dw_classes = dynamic_world_classifications_monthly.first().bandNames()\n",
    "dw_classes_str = dw_classes.getInfo()\n",
    "\n",
    "#Get dictionary of classes and values\n",
    "#Define array of land cover classification values\n",
    "dw_class_values = np.arange(1,10).tolist()\n",
    "dw_class_values = ee.List(dw_class_values)\n",
    "#Create dictionary representing land cover classes and land cover class values\n",
    "dw_classes_dict = ee.Dictionary.fromLists(dw_classes, dw_class_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define color palettes to map land cover</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_detection_palette = ['#ffffff', # no_data=0\n",
    "                              '#419bdf', # water=1\n",
    "                              '#397d49', # trees=2\n",
    "                              '#88b053', # grass=3\n",
    "                              '#7a87c6', # flooded_vegetation=4\n",
    "                              '#e49535', # crops=5\n",
    "                              '#dfc25a', # scrub_shrub=6\n",
    "                              '#c4291b', # builtup=7\n",
    "                              '#a59b8f', # bare_ground=8\n",
    "                              '#a8ebff', # snow_ice=9\n",
    "                              '#616161', # clouds=10\n",
    "]\n",
    "statesViz = {'min': 0, 'max': 10, 'palette': change_detection_palette};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Gather projection and geometry information from the land cover classifications</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = dynamic_world_classifications_monthly.first().projection().getInfo()\n",
    "crs = projection.get('crs')\n",
    "crsTransform = projection.get('transform')\n",
    "scale = dynamic_world_classifications_monthly.first().projection().nominalScale().getInfo()\n",
    "print('CRS and Transform: ',crs, crsTransform)\n",
    "\n",
    "geometry = dynamic_world_classifications_monthly.first().geometry().bounds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define years to assess land cover and convert monthly classifications to annual classifications</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Define years to get annual classifications for\n",
    "years = np.arange(2016,2020)\n",
    "\n",
    "#Squash scenes from monthly to annual for both the annual classification and annual probabilities\n",
    "dynamic_world_classifications = npv.squashScenesToAnnualClassification(dynamic_world_classifications_monthly,years,method='median')\n",
    "dynamic_world_annual_probabilities = npv.squashScenesToAnnualProbability(dynamic_world_classifications_monthly,years,method='median',image_prefix='prob_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define consistent change image to sample training points. Here we will define consistent change as:\n",
    "* $$\\ state(i) != state(i+1) $$\n",
    "* $$\\ state(i+1) = state(i+2) $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert classifications to an image with band names as original image names\n",
    "dynamic_world_classification_image = dynamic_world_classifications.toBands()\n",
    "\n",
    "#Get a collection of images where each image has 3 bands: classifications for year(i), classifications for year(i+1), and classifications for year i+2\n",
    "lc_consistent_change_one_year_col = npv.getYearStackIC(dynamic_world_classification_image,dynamic_world_classification_image.bandNames().getInfo(), band_indices=[0,1,2])\n",
    "\n",
    "#Get a collection of images where each image represents whether change was consistent from year(i) to year(i+1)\n",
    "lc_consistent_change_one_year_col = lc_consistent_change_one_year_col.map(npv.LC_ConsistentChangeOneYear)\n",
    "\n",
    "#Convert this collection to one image\n",
    "lc_consistent_change_one_year_image = lc_consistent_change_one_year_col.toBands()\n",
    "\n",
    "#Rename image bands to the central year\n",
    "lc_consistent_change_one_year_image = lc_consistent_change_one_year_image.rename(lc_consistent_change_one_year_col.aggregate_array('OriginalBand'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Now *lc_consistent_change_one_year_image* is a binary multi-band image where each band represents\n",
    "whether change from $year_{i}$ to $year_{i+1}$ is consistent\n",
    "\n",
    "However it also includes 0's for where there was no change from $year_{i}$ to $year_{i+1}$, therefore we need to\n",
    "mask the *lc_consistent_change_one_year_image* to only pixels with change form $year_{i}$ to $year_{i+1}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Mask consistent change image to pixels with change in that year</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a collection of images where each image has 2 bands: classifications for year(i) and classifications for year(i+1)\n",
    "lc_one_change_col = npv.getYearStackIC(dynamic_world_classification_image,dynamic_world_classification_image.bandNames().getInfo(), band_indices=[0,1])\n",
    "\n",
    "#Get a collection of images where each image represents whether there was change from year(i) to year(i+1)\n",
    "lc_one_change_col = lc_one_change_col.map(npv.LC_OneChange)\n",
    "\n",
    "#Convert this collection to one image\n",
    "lc_one_change_image = lc_one_change_col.toBands()\n",
    "\n",
    "#Rename image bands to the central year\n",
    "lc_one_change_image = lc_one_change_image.rename(lc_one_change_col.aggregate_array('OriginalBand'))\n",
    "\n",
    "#Mask each band of the consistent change image to \n",
    "lc_consistent_change_one_year_image = lc_consistent_change_one_year_image.mask(lc_one_change_image.select(lc_consistent_change_one_year_image.bandNames()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Map three years worth of land cover and the consistent change image to check *lc_consistent_change_one_year_image*\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneChangeDetectionViz = {'min': 0, 'max': 1, 'palette': ['696a76','ff2b2b']}; #gray = 0, red = 1\n",
    "consistentChangeDetectionViz = {'min': 0, 'max': 1, 'palette': ['0741df','df07b5']}; #blue = 0, pink = 1\n",
    "\n",
    "\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map1 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map1.addLayer(dynamic_world_classification_image.select('2017_class'),statesViz,name='2017 LC')\n",
    "Map1.addLayer(dynamic_world_classification_image.select('2018_class'),statesViz,name='2018 LC')\n",
    "Map1.addLayer(dynamic_world_classification_image.select('2019_class'),statesViz,name='2019 LC')\n",
    "Map1.addLayer(lc_one_change_image.select('2017_class'),oneChangeDetectionViz,name='One Change from 2017-2018')\n",
    "Map1.addLayer(lc_consistent_change_one_year_image.select('2017_class'),consistentChangeDetectionViz,\n",
    "              name='Consistent Change from 2017-2019')\n",
    "display(Map1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first layer will show classifications for 2017\n",
    "* The second layer will show classifications for 2018\n",
    "* The third layer will show classifications for 2019\n",
    "* The fourth layer will show whether change occured from 2017 to 2018, with grey for no and red for yes\n",
    "* The fifth layer will show whether change from 2017 to 2018 stayed consistent in 2019, with blue for no and pink for yes\n",
    "* The final layer should be masked to show only consistent change in the red areas of the fourth layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Gather neighborhood predictor variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Apply land cover change functions to images, first returns an image collection then converted to image\n",
    "#where each band represents one year\n",
    "\n",
    "#Get band names\n",
    "dwc_bandNames = dynamic_world_classification_image.bandNames().getInfo()\n",
    "\n",
    "#Pixels where state(i-1) != state(i) and state(i-1) = state(i+1)\n",
    "lc_reversed_col = npv.getYearStackIC(dynamic_world_classification_image, dwc_bandNames, band_indices=[-1,0,1])\n",
    "lc_reversed_col = lc_reversed_col.map(npv.LC_Reverse)\n",
    "lc_reversed_image = lc_reversed_col.toBands()\n",
    "lc_reversed_image = lc_reversed_image.rename(lc_reversed_col.aggregate_array('OriginalBand'))\n",
    "lc_reversed_image = lc_reversed_image.set('system:index','LC_Reversed')\n",
    "\n",
    "#Pixels where state(i-1) != state(i) and state(i) != state(i+1) and state(i-1) != state(i+1)\n",
    "lc_changed_to_another_col = npv.getYearStackIC(dynamic_world_classification_image, dwc_bandNames,band_indices=[-1,0,1])\n",
    "lc_changed_to_another_col = lc_changed_to_another_col.map(npv.LC_ChangeToAnother)\n",
    "lc_changed_to_another_image = lc_changed_to_another_col.toBands()\n",
    "lc_changed_to_another_image = lc_changed_to_another_image.rename(lc_changed_to_another_col.aggregate_array('OriginalBand'))\n",
    "lc_changed_to_another_image = lc_changed_to_another_image.set('system:index','LC_ChangedToAnother')\n",
    "\n",
    "\n",
    "#Get binary images of the land cover classifications for the following year\n",
    "lc_year_after_col = npv.getYearStackIC(dynamic_world_classification_image, dwc_bandNames, band_indices=[0,1])\n",
    "lc_year_after_col = lc_year_after_col.map(npv.LC_YearAfter)\n",
    "lc_year_after_image = lc_year_after_col.toBands()\n",
    "lc_year_after_image = lc_year_after_image.rename(lc_year_after_col.aggregate_array('OriginalBand'))\n",
    "lc_year_after_image = npv.convertClassificationsToBinaryImages(lc_year_after_image, dw_classes_dict)\n",
    "\n",
    "#Get binary images of the land cover classifications for the current year\n",
    "lc_year_image = npv.convertClassificationsToBinaryImages(dynamic_world_classification_image, dw_classes_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have mutliple options for predictor variables including:\n",
    "1. How many pixels from $year_{i-1}$ to $year_{i}$ changed, then from $year_{i}$ to $year_{i+1}$ flipped back to $state_{i-1}$\n",
    "2. How many pixels from $year_{i-1}$ to $year_{i}$ changed, then from $year_{i}$ to $year_{i+1}$ changed to a different state than $state_{i-1}$\n",
    "3. How many pixels changed from $year_{i}$ to $year_{i+1}$\n",
    "4. How many pixels are of each class in $year_{i}$\n",
    "5. How many pixels are of each class in $year_{i+1}$\n",
    "6. The average probability of each class in $year_{i}$\n",
    "7. The average probability of each class in $year_{i+1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">For the remainder of the notebook, we'll sample training points and predictor variables for one year only.\n",
    "\n",
    "Gather predictor variables for 2017, and build a list of predictor variables and defined column names</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_select = '2017_class'\n",
    "year_after_to_select = '2018_class'\n",
    "lc_year_after_image_select = lc_year_after_image.filterMetadata('system:index','equals',year_to_select).first()\n",
    "lc_year_image_select = lc_year_image.filterMetadata('system:index','equals',year_to_select).first()\n",
    "\n",
    "current_year_probs = dynamic_world_annual_probabilities.filterMetadata('system:index','equals','prob_2017').first()\n",
    "current_year_probs = current_year_probs.set('system:index','current_year_prob')\n",
    "\n",
    "following_year_probs = dynamic_world_annual_probabilities.filterMetadata('system:index','equals','prob_2018').first()\n",
    "following_year_probs = current_year_probs.set('system:index','following_year_prob')\n",
    "\n",
    "#We'll only select the \"one_change\", \"reversed\", \"changed to another\", probabilities for the current year,\n",
    "#and probabilities for the following year as the predictor variables to test\n",
    "predictor_variable_list = [lc_one_change_image.select(year_to_select),\n",
    "                           lc_reversed_image.select(year_to_select),\n",
    "                           lc_changed_to_another_image.select(year_to_select),\n",
    "                           current_year_probs,\n",
    "                           following_year_probs]\n",
    "predictor_variable_image = ee.ImageCollection(predictor_variable_list).toBands()\n",
    "\n",
    "#Rename bands to more intelligible ones\n",
    "predictor_variable_names = ['LC_OneChange','LC_Reversed','LC_ChangedToAnother']+['current_year_{}'.format(x) for x in current_year_probs.bandNames().getInfo()]+['following_year_{}'.format(x) for x in following_year_probs.bandNames().getInfo()]\n",
    "predictor_variable_image = predictor_variable_image.rename(predictor_variable_names)\n",
    "\n",
    "# #If you want to select the \"how many pixels of each class\" for the current year and following year, you can use the line below to rename predictor_variable_image for those images\n",
    "# #['current_year_{}'.format(x) for x in dw_classes_dict.keys().getInfo()]+['following_year_{}'.format(x) for x in dw_classes_dict.keys().getInfo()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Kernels to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Here are are some sample kernels you can try, we'll be testing the performance of each of them\n",
    "fixed_kernel = ee.Kernel.fixed(3,3,\n",
    "                         [[1,1,1],\n",
    "                          [1,0,1],\n",
    "                          [1,1,1]]\n",
    "                          ,1,1)\n",
    "\n",
    "gaussian_kernel = ee.Kernel.gaussian(radius=1000, units='meters', sigma=1000)\n",
    "\n",
    "circle_kernel = ee.Kernel.circle(radius=17, units='pixels')\n",
    "\n",
    "square_kernel = ee.Kernel.square(radius=1.5, units='pixels')\n",
    "\n",
    "\n",
    "#Convolve the predictor variable image to get the neighborhood values of each predictor variable\n",
    "fixed_kernel_predictors = predictor_variable_image.convolve(fixed_kernel)\n",
    "gaussian_kernel_predictors = predictor_variable_image.convolve(gaussian_kernel)\n",
    "circle_kernel_predictors = predictor_variable_image.convolve(circle_kernel)\n",
    "square_kernel_predictors = predictor_variable_image.convolve(square_kernel)\n",
    "\n",
    "#Now to easily loop over the kernels through this process, we'll put them into a dictionary. \n",
    "kernel_dictionary = {\n",
    "    'fixed_kernel':fixed_kernel_predictors,\n",
    "    'gaussian_kernel': gaussian_kernel_predictors,\n",
    "    'circle_kernel':circle_kernel_predictors,\n",
    "    'square_kernel':square_kernel_predictors,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Gather Training Points\n",
    "\n",
    "<font size=\"4\">Now that we have a binary image of where consistent change occurs (masked to areas of change) and our convolved predictor variables, we'll build our training, validation, and test set. We'll only sample training points from one year to simplify the process, however you can sample locations from multiple years.\n",
    "\n",
    "First we'll sample the training point locations by taking stratified samples of the consistent change image, then we'll sample the predictor variables at those locations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#We'll remove training points that are within a certain distance to the validation and test points\n",
    "#However since we're testing over a small area, we'll keep the distance small\n",
    "distanceToFilter = 100\n",
    "\n",
    "\n",
    "#Select consistent change for 2017\n",
    "lc_consistent_change_one_year_select = lc_consistent_change_one_year_image.select(year_to_select).rename('Consistent Change')\n",
    "\n",
    "#Sample point locations that we will split into training, validation, and test sets\n",
    "point_locations = npv.getStratifiedSampleBandPoints(lc_consistent_change_one_year_select, region=geometry, \n",
    "                                                       numPoints=2000, bandName='Consistent Change',seed=num_seed,\n",
    "                                                       geometries=True,scale=scale, projection=crs)\n",
    "\n",
    "#Assign random value between 0 and 1 to split into training, validation, and test sets\n",
    "point_locations = point_locations.randomColumn(columnName='TrainingSplit', seed=num_seed)\n",
    "#First split training set from the rest, taking 70% of the points for training\n",
    "#Roughly 70% training, 30% for validation + testing\n",
    "training_split = 0.7\n",
    "training_locations = point_locations.filter(ee.Filter.lt('TrainingSplit', training_split))\n",
    "validation_and_test = point_locations.filter(ee.Filter.gte('TrainingSplit', training_split))\n",
    "\n",
    "#Define distance filter to remove training points within a certain distance of test points and validation points\n",
    "distFilter = ee.Filter.withinDistance(distance=distanceToFilter, leftField='.geo', rightField= '.geo', maxError= 1)\n",
    "join = ee.Join.inverted()\n",
    "training_locations = join.apply(training_locations, validation_and_test, distFilter);\n",
    "\n",
    "#Assign another random value between 0 and 1 to validation_and_test to split to validation and test sets\n",
    "validation_and_test = validation_and_test.randomColumn(columnName='ValidationSplit', seed=num_seed)\n",
    "#Of the 30% saved for validation + testing, half goes to validation and half goes to test\n",
    "#Meaning original sample will be 70% training, 15% validation, 15% testing\n",
    "validation_split = 0.5 \n",
    "validation_locations = validation_and_test.filter(ee.Filter.lt('ValidationSplit', validation_split))\n",
    "test_locations = validation_and_test.filter(ee.Filter.gte('ValidationSplit', validation_split))\n",
    "\n",
    "#Apply distance filter to remove validation points within a certain distance of test points\n",
    "validation_locations = join.apply(validation_locations, test_locations, distFilter);\n",
    "\n",
    "#Export these locations to an Earth Engine asset\n",
    "location_description = '{}_locations'\n",
    "location_assetID = 'projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=training_locations, \n",
    "    description = location_description.format('training'), \n",
    "    assetId = location_assetID.format('training'))\n",
    "export_results_task.start()\n",
    "    \n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=validation_locations, \n",
    "    description = location_description.format('validation'), \n",
    "    assetId = location_assetID.format('validation'))\n",
    "export_results_task.start()\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=test_locations, \n",
    "    description = location_description.format('test'), \n",
    "    assetId = location_assetID.format('test'))\n",
    "export_results_task.start()\n",
    " \n",
    "#Wait for last export to finish\n",
    "while export_results_task.active():\n",
    "    print('Polling for task (id: {}).'.format(export_results_task.id))\n",
    "    time.sleep(30)\n",
    "print('Done with export.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Next we'll sample the neighborhood predictor variables for each of the neighborhoods for the training, validation, and test sets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define properties to export to an Earth Engine asset\n",
    "points_description = '{}_{}_points'\n",
    "points_assetID = 'projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_{}_points'\n",
    "\n",
    "#Load the point locations assets\n",
    "training_locations_asset = ee.FeatureCollection('projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'.format('training'))\n",
    "validation_locations_asset = ee.FeatureCollection('projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'.format('validation'))\n",
    "test_locations_asset = ee.FeatureCollection('projects/wri-datalab/DynamicWorld_CD/ModelResults/Neighborhood_{}_locations'.format('test'))\n",
    "\n",
    "\n",
    "#Iterate through kernel dictionary\n",
    "for key,value in kernel_dictionary.items():\n",
    "    #Sample the neighborhood image at point locations \n",
    "    training_points = value.sampleRegions(training_locations_asset, scale=scale, projection=crs, geometries=True,tileScale=2)\n",
    "    validation_points = value.sampleRegions(validation_locations_asset, scale=scale, projection=crs, geometries=True,tileScale=2)\n",
    "    test_points = value.sampleRegions(test_locations_asset, scale=scale, projection=crs, geometries=True,tileScale=2)\n",
    "    \n",
    "    export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=training_points, \n",
    "        description = points_description.format(key,'training'), \n",
    "        assetId = points_assetID.format(key,'training'))\n",
    "    export_results_task.start()\n",
    "    \n",
    "    export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=validation_points, \n",
    "        description = points_description.format(key,'validation'), \n",
    "        assetId = points_assetID.format(key,'validation'))\n",
    "    export_results_task.start()\n",
    "    \n",
    "    export_results_task = ee.batch.Export.table.toAsset(\n",
    "        collection=test_points, \n",
    "        description = points_description.format(key,'test'), \n",
    "        assetId = points_assetID.format(key,'test'))\n",
    "    export_results_task.start()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: define models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Define dictionaries of parameters and models to test\n",
    "#You can find the inputs for the parameters under the ee.Classifiers section of GEE\n",
    "\n",
    "rf_parameters = {'seed':[num_seed], \n",
    "          'numberOfTrees': [50,100], \n",
    "          'variablesPerSplit': [4,8,10,None], \n",
    "          'minLeafPopulation': [None,10,50], \n",
    "          'bagFraction': [None,0.5,.3], \n",
    "          'maxNodes': [None, 20, 50]\n",
    "         }\n",
    "#buildGridSearchList converts the parameter dictionary into a list of classifiers that can be used in cross-validation\n",
    "rf_classifier_list = gclass.buildGridSearchList(rf_parameters,'smileRandomForest')\n",
    "\n",
    "svm_parameters = {'decisionProcedure':[None]}\n",
    "svm_classifier_list = gclass.buildGridSearchList(svm_parameters,'libsvm')\n",
    "\n",
    "maxent_parameters = {'minIterations':[10,100],\n",
    "                    'maxIterations':[50,200]}\n",
    "maxent_classifier_list = gclass.buildGridSearchList(maxent_parameters,'gmoMaxEnt')\n",
    "\n",
    "classifier_list = rf_classifier_list+svm_classifier_list+maxent_classifier_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Use cross validation to choose optimal parameters for each model, kernel option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name y_column\n",
    "y_column = 'Consistent Change'\n",
    "\n",
    "#Define assetId and description format to export to GEE\n",
    "cv_results_assetId = 'projects/wri-datalab/DynamicWorld_CD/ModelResults/cv_export_{}'\n",
    "cv_results_description = 'cv_export_{}'\n",
    "\n",
    "#Loop through kernel_dictionary to perform cross validation\n",
    "for key, value in kernel_dictionary.items():\n",
    "    #Try statement only because the training set export takes a while for some neighborhoods\n",
    "    try:\n",
    "        #Load training points\n",
    "        training_points = ee.FeatureCollection(points_assetID.format(key,'training'))\n",
    "\n",
    "        #Perform cross validation, returns a feature collection\n",
    "        cv_results = gclass.kFoldCrossValidation(inputtedFeatureCollection = training_points, \n",
    "                                             propertyToPredictAsString = y_column, \n",
    "                                             predictors = predictor_variable_names, \n",
    "                                             listOfClassifiers = classifier_list,\n",
    "                                             k=3,seed=num_seed)\n",
    "        #Export results to GEE\n",
    "        export_results_task = ee.batch.Export.table.toAsset(\n",
    "                collection=cv_results, \n",
    "                description = cv_results_description.format(key), \n",
    "                assetId = cv_results_assetId.format(key))\n",
    "        export_results_task.start()\n",
    "    except:\n",
    "        None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Load cross validation results and find the best model based on accuracy on the validation set</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty dataframe to save results of cross validation\n",
    "accuracy_and_keys = pd.DataFrame()\n",
    "\n",
    "#Loop through kernels\n",
    "for key, value in kernel_dictionary.items():\n",
    "    \n",
    "    #Load cross-validation results\n",
    "    results = ee.FeatureCollection(cv_results_assetId.format(key))\n",
    "    #Get the best result by the cross validation score\n",
    "    best_result = results.sort('Validation Score', False).first()\n",
    "    \n",
    "    #Load params as a dictionary\n",
    "    params = best_result.get('Params').getInfo()\n",
    "    params = ast.literal_eval(params)\n",
    "    \n",
    "    #Get the calssifier name\n",
    "    classifierName = best_result.get('Classifier Type').getInfo()\n",
    "    \n",
    "    #Load classifier with best params\n",
    "    classifier = gclass.defineClassifier(params,classifierName)\n",
    "\n",
    "    #Load training and validation points\n",
    "    training_points = ee.FeatureCollection(points_assetID.format(key,'training'))\n",
    "    validation_points = ee.FeatureCollection(points_assetID.format(key,'validation'))\n",
    "    \n",
    "    #Train a classifier with the best params on the training data\n",
    "    classifier = classifier.train(training_points, classProperty=y_column, \n",
    "                                  inputProperties=predictor_variable_names, subsamplingSeed=num_seed)\n",
    "    \n",
    "    #Predict over validation data\n",
    "    validation_points_predicted = validation_points.classify(classifier)\n",
    "    \n",
    "    #Get confusion matrix and accuracy score\n",
    "    confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "    accuracy = confusion_matrix.accuracy().getInfo()\n",
    "\n",
    "    #Save results to dataframe\n",
    "    results_dict = {'key':key,'accuracy':accuracy,'Params':params,'Classifier Type':classifierName}\n",
    "    accuracy_and_keys = accuracy_and_keys.append(results_dict, ignore_index=True)\n",
    "    \n",
    "    #Print results\n",
    "    print('Kernel Name',key, 'Classifier',classifierName)\n",
    "    print(gclass.pretty_print_confusion_matrix(confusion_matrix.getInfo()))\n",
    "    print('Accuracy',accuracy)\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "#Get the best model\n",
    "best_model_row = accuracy_and_keys.sort_values(by=['accuracy'],ascending=False).iloc[0]\n",
    "#Set it up as a GEE Classifier\n",
    "best_model = gclass.defineClassifier(best_model_row['Params'],best_model_row['Classifier Type'])\n",
    "best_model = best_model.train(training_points, classProperty=y_column, \n",
    "                                  inputProperties=predictor_variable_names, subsamplingSeed=num_seed)\n",
    "#Predict over validation data\n",
    "validation_points_predicted = validation_points.classify(best_model)\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = validation_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "print('--------------------------------------------------------------')\n",
    "print('Final Model Validation Set Confusion Matrix')\n",
    "print(gclass.pretty_print_confusion_matrix(confusion_matrix.getInfo()))\n",
    "print('Final Model Validation Set Accuracy',accuracy)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#Predict over test data\n",
    "test_points = ee.FeatureCollection(points_assetID.format(key,'test'))\n",
    "\n",
    "test_points_predicted = test_points.classify(best_model)\n",
    "\n",
    "#Get confusion matrix and accuracy score\n",
    "confusion_matrix = test_points_predicted.errorMatrix(y_column, 'classification');\n",
    "accuracy = confusion_matrix.accuracy().getInfo()\n",
    "print('--------------------------------------------------------------')\n",
    "print('Final Model Test Set Confusion Matrix')\n",
    "print(gclass.pretty_print_confusion_matrix(confusion_matrix.getInfo()))\n",
    "print('Final Model Test Set Accuracy',accuracy)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Use model to predict consistent change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use best model and predictor variable image to predict consistent change\n",
    "predicted_consistent_change = predictor_variable_image.updateMask(lc_one_change_image.select(year_to_select)).classify(best_model)\n",
    "\n",
    "#The actual consistent change was saved to a variable lc_consistent_change_one_year_select earlier\n",
    "\n",
    "#Map the results\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map2 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map2.addLayer(dynamic_world_classification_image.select('2017_class'),statesViz,name='2017 LC')\n",
    "Map2.addLayer(dynamic_world_classification_image.select('2018_class'),statesViz,name='2018 LC')\n",
    "Map2.addLayer(dynamic_world_classification_image.select('2019_class'),statesViz,name='2019 LC')\n",
    "Map2.addLayer(lc_consistent_change_one_year_select,consistentChangeDetectionViz,\n",
    "              name='Consistent Change from 2017-2019')\n",
    "Map2.addLayer(predicted_consistent_change,oneChangeDetectionViz,\n",
    "              name='Predicted Consistent Change from 2017-2018')\n",
    "\n",
    "display(Map2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * The first layer will show classifications for 2017\n",
    "#### * The second layer will show classifications for 2018\n",
    "#### * The third layer will show classifications for 2019\n",
    "#### * The fourth layer will show whether change from 2017 to 2018 stayed consistent in 2019, with grey for no and red for yes with blue for no and pink for yes\n",
    "#### * The fifth layer will show the model's prediction on whether change from 2017 to 2018 stayed consistent in 2019, blue for no and pink for yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can view/save the model using the \"explain\" function of classifiers\n",
    "classifierString = best_model.explain().get('trees')\n",
    "print(classifierString.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
