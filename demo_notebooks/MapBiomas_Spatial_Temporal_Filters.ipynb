{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the use of post-classification filters created by the MapBiomas team.\n",
    "The MapBiomas team provides guidance on post-classification filters, including gap filling, spatial filters, temporal filters, incidence filters, and frequency filters. These filters were implemented in the module [post_classification_filters.py] of the [wri_change_detection repository](https://github.com/wri/rw-dynamicworld-cd/tree/master/wri_change_detection). This notebook is provided to give a tutorial on how to apply these filters to a land cover classification time series over one area of Brazil, classified using Dynamic World.\n",
    "\n",
    "You can learn more about the MapBiomas project at their [home page](https://mapbiomas.org/). The development of MapBiomas was done by several groups for each biome and cross-cutting theme that occurs in Brazil. You can read more of the methodology in the [Algorithm Theoretical Basis Document (ATBD) Page](https://mapbiomas.org/en/download-of-atbds) on their website, including the main ATBD and appendices for each each biome and cross-cutting themes. \n",
    "\n",
    "From Section 3.5 of the ATBD, MapBiomas defines post-classification filters,\n",
    "\"[due] to the pixel-based classification method and the long temporal series, a chain of post-classification filters was applied. The first post-classification action involves the application of temporal filters. Then, a spatial filter was applied followed by a gap fill filter. The application of these filters remove classification noise. \n",
    "These post-classification procedures were implemented in the Google Earth Engine platform\"\n",
    "\n",
    "Below is the copy of the licensing for MapBiomas:\n",
    "The MapBiomas data are public, open and free through license Creative Commons CC-CY-SA and the simple reference of the source observing the following format:\n",
    "\"Project MapBiomas - Collection v5.0 of Brazilian Land Cover & Use Map Series, accessed on 12/14/2020 through the link: https://github.com/mapbiomas-brazil/mapbiomas-brazil.github.io\"\n",
    "\"MapBiomas Project - is a multi-institutional initiative to generate annual land cover and use maps using automatic classification processes applied to satellite images. The complete description of the project can be found at http://mapbiomas.org\".\n",
    "Access here the scientific publication: Souza at. al. (2020) - Reconstructing Three Decades of Land Use and Land Cover Changes in Brazilian Biomes with Landsat Archive and Earth Engine - Remote Sensing, Volume 12, Issue 17, 10.3390/rs12172735.\n",
    "\n",
    "\n",
    "This notebook includes 6 Steps:\n",
    "1. Loading land cover classifications from Dynamic World\n",
    "2. Applying Gap Filling for Clouds\n",
    "3. Applying Temporal Filters\n",
    "4. Applying Spatial Filters\n",
    "5. Applying Incidence Filters\n",
    "5. Applying Frequency Filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and iniatilize Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Map, basemaps\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# relative import for this folder hierarchy, credit: https://stackoverflow.com/a/35273613\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from wri_change_detection import preprocessing as npv\n",
    "from wri_change_detection import gee_classifier as gclass\n",
    "from wri_change_detection import post_classification_filters as pcf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Iniatilize Earth Engine and Google Cloud authentication</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize earth engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define a seed number to ensure reproducibility across random processes. This seed will be used in all subsequent sampling as well. We'll also define seeds for sampling the training, validation, and test sets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seed=30\n",
    "random.seed(num_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Land Cover Classifications and Label Data\n",
    "\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "Define land cover classification image collection, with one image for each time period. Each image should have one band representing the classification in that pixel for one time period.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load collection\n",
    "#This collection represents monthly dynamic world classifications of land cover, later we'll squash it to annual\n",
    "dynamic_world_classifications_monthly = ee.ImageCollection('projects/wings-203121/assets/dynamic-world/v3-5_stack_tests/wri_test_goldsboro')\n",
    "\n",
    "#Get classes from first image\n",
    "dw_classes = dynamic_world_classifications_monthly.first().bandNames()\n",
    "dw_classes_str = dw_classes.getInfo()\n",
    "full_dw_classes_str = ['No Data']+dw_classes_str\n",
    "\n",
    "#Get dictionary of classes and values\n",
    "#Define array of land cover classification values\n",
    "dw_class_values = np.arange(1,10).tolist()\n",
    "dw_class_values_ee = ee.List(dw_class_values)\n",
    "#Create dictionary representing land cover classes and land cover class values\n",
    "dw_classes_dict = ee.Dictionary.fromLists(dw_classes, dw_class_values_ee)\n",
    "\n",
    "#Make sure the dictionary looks good\n",
    "print(dw_classes_dict.getInfo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Define color palettes to map land cover</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_detection_palette = ['#ffffff', # no_data=0\n",
    "                              '#419bdf', # water=1\n",
    "                              '#397d49', # trees=2\n",
    "                              '#88b053', # grass=3\n",
    "                              '#7a87c6', # flooded_vegetation=4\n",
    "                              '#e49535', # crops=5\n",
    "                              '#dfc25a', # scrub_shrub=6\n",
    "                              '#c4291b', # builtup=7\n",
    "                              '#a59b8f', # bare_ground=8\n",
    "                              '#a8ebff', # snow_ice=9\n",
    "                              '#616161', # clouds=10\n",
    "]\n",
    "statesViz = {'min': 0, 'max': 10, 'palette': change_detection_palette};\n",
    "\n",
    "oneChangeDetectionViz = {'min': 0, 'max': 1, 'palette': ['696a76','ff2b2b']}; #gray = 0, red = 1\n",
    "consistentChangeDetectionViz = {'min': 0, 'max': 1, 'palette': ['0741df','df07b5']}; #blue = 0, pink = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Gather projection and geometry information from the land cover classifications</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_ee = dynamic_world_classifications_monthly.first().projection()\n",
    "projection = projection_ee.getInfo()\n",
    "crs = projection.get('crs')\n",
    "crsTransform = projection.get('transform')\n",
    "scale = dynamic_world_classifications_monthly.first().projection().nominalScale().getInfo()\n",
    "print('CRS and Transform: ',crs, crsTransform)\n",
    "\n",
    "geometry = dynamic_world_classifications_monthly.first().geometry().bounds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Convert the land cover collection to a multiband image, one band for each year</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Define years to get annual classifications for\n",
    "years = np.arange(2016,2020)\n",
    "\n",
    "#Squash scenes from monthly to annual\n",
    "dynamic_world_classifications = npv.squashScenesToAnnualClassification(dynamic_world_classifications_monthly,years,method='median',image_name='dw_{}')\n",
    "#Get image names \n",
    "dw_band_names = dynamic_world_classifications.aggregate_array('system:index').getInfo()\n",
    "#Convert to a multiband image and rename using dw_band_names\n",
    "dynamic_world_classifications_image = dynamic_world_classifications.toBands().rename(dw_band_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Load label data to later compare land cover classification to label data. Export points of labelled data in order to compare to classifications later.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only labels for regions in Modesto CA, Goldsboro NC, the Everglades in FL, and one region in Brazil have been\n",
    "#uploaded to this collection\n",
    "labels = ee.ImageCollection('projects/wri-datalab/DynamicWorld_CD/DW_Labels')\n",
    "\n",
    "#Filter to where we have DW classifications\n",
    "labels_filtered = labels.filterBounds(dynamic_world_classifications_monthly.geometry())\n",
    "print('Number of labels that overlap classifications', labels_filtered.size().getInfo())\n",
    "\n",
    "#Save labels projection\n",
    "labels_projection = labels_filtered.first().projection()\n",
    "#Define geometry to sample points from \n",
    "labels_geometry = labels_filtered.geometry().bounds()\n",
    "\n",
    "#Compress labels by majority vote\n",
    "labels_filtered = labels_filtered.reduce(ee.Reducer.mode())\n",
    "#Remove pixels that were classified as no data\n",
    "labels_filtered = labels_filtered.mask(labels_filtered.neq(0))\n",
    "#Rename band\n",
    "labels_filtered = labels_filtered.rename(['labels'])\n",
    "\n",
    "\n",
    "#Sample points from label image at every pixel\n",
    "labelPoints = labels_filtered.sample(region=labels_geometry, projection=labels_projection, \n",
    "                                     factor=1, \n",
    "                                     seed=num_seed, dropNulls=True,\n",
    "                                     geometries=True)\n",
    "\n",
    "#Export sampled points\n",
    "labelPoints_export_name = 'goldsboro'\n",
    "labelPoints_assetID = 'projects/wri-datalab/DynamicWorld_CD/DW_LabelPoints_{}'\n",
    "labelPoints_description = 'DW_LabelPoints_{}'\n",
    "\n",
    "export_results_task = ee.batch.Export.table.toAsset(\n",
    "    collection=labelPoints, \n",
    "    description = labelPoints_description.format(labelPoints_export_name), \n",
    "    assetId = labelPoints_assetID.format(labelPoints_export_name))\n",
    "export_results_task.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Map years to check them out.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map years to check them out!\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map1 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map1.addLayer(dynamic_world_classifications_image.select('dw_2016'),statesViz,name='2016 DW LC')\n",
    "Map1.addLayer(dynamic_world_classifications_image.select('dw_2017'),statesViz,name='2017 DW LC')\n",
    "Map1.addLayer(dynamic_world_classifications_image.select('dw_2018'),statesViz,name='2018 DW LC')\n",
    "Map1.addLayer(dynamic_world_classifications_image.select('dw_2019'),statesViz,name='2019 DW LC')\n",
    "Map1.addLayer(labels_filtered,statesViz,name='Labels')\n",
    "display(Map1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Calculate Accuracy and Confusion Matrix for Original Classifications on Label Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label points\n",
    "labelPointsFC = ee.FeatureCollection(labelPoints_assetID.format('goldsboro'))\n",
    "\n",
    "#Save 2019 DW classifications and rename to \"dw_classifications\"\n",
    "dw_2019 = dynamic_world_classifications_image.select('dw_2019').rename('dw_classifications')\n",
    "\n",
    "#Sample the 2019 classifications at each label point\n",
    "labelPointsWithDW = dw_2019.sampleRegions(collection=labelPointsFC, projection = projection_ee, \n",
    "                                          tileScale=4, geometries=True)\n",
    "\n",
    "#Calculate confusion matrix, which we will use for an accuracy assessment\n",
    "originalErrorMatrix = labelPointsWithDW.errorMatrix('labels', 'dw_classifications')\n",
    "\n",
    "#Print the confusion matrix with the class names as a dataframe\n",
    "errorMatrixDf = gclass.pretty_print_confusion_matrix_multiclass(originalErrorMatrix, full_dw_classes_str)\n",
    "#Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.\n",
    "print('Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.')\n",
    "display(errorMatrixDf)\n",
    "\n",
    "#You can also print further accuracy scores from the confusion matrix, however each one takes a couple minutes \n",
    "#to load\n",
    "print('Accuracy',originalErrorMatrix.accuracy().getInfo())\n",
    "# print('Consumers Accuracy',originalErrorMatrix.consumersAccuracy().getInfo())\n",
    "# print('Producers Accuracy',originalErrorMatrix.producersAccuracy().getInfo())\n",
    "# print('Kappa',originalErrorMatrix.kappa().getInfo())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of changes for each year\n",
    "\n",
    "for year in years[0:-1]:\n",
    "    year_list = ['dw_{}'.format(year),'dw_{}'.format(year+1)]\n",
    "    num_changes = pcf.calculateNumberOfChanges(dynamic_world_classifications_image.select(year_list), year_list)\n",
    "\n",
    "    num_changes_mean = num_changes.reduceRegion(reducer=ee.Reducer.mean(), \n",
    "                                                  geometry=geometry,\n",
    "                                                  crs=crs, crsTransform=crsTransform, \n",
    "                                                  bestEffort=True, \n",
    "                                                  maxPixels=1e13, tileScale=4)\n",
    "    print('Number of changes from',year,'to',year+1,\"{:.4f}\".format(num_changes_mean.get('sum').getInfo()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Applying Filters\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "Now that we have prepared all of the necessary variables to do our post-processing, we'll start applying the filters defined by MapBiomas. While the filters are designed to be applied serially, here we'll apply each filter individually (after the gap filling) in order to see the performance of each one on its own, mainly because we only have so many years of Dynamic World. For each filter, we'll apply the filter, then find the overall accuracy against the training data. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply Gap Filling\n",
    "\n",
    "\n",
    "<font size=\"4\">Section 3.5.1. of the ATBD: Gap fill:\n",
    "The Gap fill filter was used to fill possible no-data values. In a long time series of severely cloud-affected regions, it is expected that no-data values may populate some of the resultant median composite pixels. In this filter, no-data values (“gaps”) are theoretically not allowed and are replaced by the temporally nearest valid classification. In this procedure, if no “future” valid position is available, then the no-data value is replaced by its previous valid class. Up to three prior years can be used to fill in persistent no-data positions. Therefore, gaps should only exist if a given pixel has been permanently classified as no-data throughout the entire temporal domain.\n",
    "\n",
    "All code for the Gap Filters was provided by the [Pampa Team](https://github.com/mapbiomas-brazil/pampa) in [this file](https://github.com/mapbiomas-brazil/pampa/blob/master/Step006_Filter_01_gagfill.js), although the same gap fill is applied to all cross-cutting themes and biome groups.\n",
    "    \n",
    "Functions were rewritten in Python and made independent of the land cover classification image. The implementation of the gap fill in the MapBiomas code actually applies both a forward no-data filter and a backwards no-data filter. \n",
    "\n",
    "For the demo Dynamic World classifications in this notebook, none of the years have any missing data! Therefore we'll introduce some fake missing data areas in order to demonstrate the gap filling.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introducing no data pixels for some years\n",
    "dw_2016_with_gaps = dynamic_world_classifications_image.select('dw_2016').mask(dynamic_world_classifications_image.select('dw_2016').neq(ee.Image.constant(3)))\n",
    "dw_2017_with_gaps = dynamic_world_classifications_image.select('dw_2017').mask(dynamic_world_classifications_image.select('dw_2017').neq(ee.Image.constant(5)))\n",
    "dw_2019_with_gaps = dynamic_world_classifications_image.select('dw_2019').mask(dynamic_world_classifications_image.select('dw_2019').neq(ee.Image.constant(1)))\n",
    "dw_with_gaps = dw_2016_with_gaps.addBands(dw_2017_with_gaps).addBands(dynamic_world_classifications_image.select('dw_2018')).addBands(dw_2019_with_gaps)\n",
    "dw_with_gaps = dw_with_gaps.rename(dw_band_names)\n",
    "\n",
    "#Apply gap filtering\n",
    "gap_filled = pcf.applyGapFilter(dw_with_gaps, dw_band_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Map the before and after to see the affects of the gap filtering</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map years to check them out!\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map2 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map2.addLayer(dw_with_gaps.select('dw_2016'),statesViz,name='2016 DW LC')\n",
    "Map2.addLayer(gap_filled.select('dw_2016'),statesViz,name='2016 Gap Filled')\n",
    "Map2.addLayer(dw_with_gaps.select('dw_2017'),statesViz,name='2017 DW LC')\n",
    "Map2.addLayer(gap_filled.select('dw_2017'),statesViz,name='2017 Gap Filled')\n",
    "Map2.addLayer(dw_with_gaps.select('dw_2018'),statesViz,name='2018 DW LC')\n",
    "Map2.addLayer(gap_filled.select('dw_2018'),statesViz,name='2018 Gap Filled')\n",
    "Map2.addLayer(dw_with_gaps.select('dw_2019'),statesViz,name='2019 DW LC')\n",
    "Map2.addLayer(gap_filled.select('dw_2019'),statesViz,name='2019 Gap Filled')\n",
    "display(Map2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Calculate accuracy and confusion matrix for gap filled classifications on label data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label points\n",
    "labelPointsFC = ee.FeatureCollection(labelPoints_assetID.format('goldsboro'))\n",
    "\n",
    "#Save 2019 post-filtered DW classifications and rename to \"dw_filterd_classifications\"\n",
    "classifications_filtered_2019 = gap_filled.select('dw_2019').rename('dw_gap_filled_classifications')\n",
    "\n",
    "#Sample the 2019 classifications at each label point\n",
    "labelPointsWithFilteredDW = classifications_filtered_2019.sampleRegions(collection=labelPointsFC, \n",
    "                                                                        projection = projection_ee, \n",
    "                                                                        tileScale=4, geometries=True)\n",
    "\n",
    "#Calculate confusion matrix, which we will use for an accuracy assessment\n",
    "filteredErrorMatrix = labelPointsWithFilteredDW.errorMatrix('labels', 'dw_gap_filled_classifications')\n",
    "\n",
    "#Print the confusion matrix with the class names as a dataframe\n",
    "errorMatrixDf = gclass.pretty_print_confusion_matrix_multiclass(filteredErrorMatrix, full_dw_classes_str)\n",
    "#Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.\n",
    "print('Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.')\n",
    "display(errorMatrixDf)\n",
    "\n",
    "#You can also print further accuracy scores from the confusion matrix, however each one takes a couple minutes \n",
    "#to load\n",
    "print('Accuracy',filteredErrorMatrix.accuracy().getInfo())\n",
    "# print('Consumers Accuracy',originalErrorMatrix.consumersAccuracy().getInfo())\n",
    "# print('Producers Accuracy',originalErrorMatrix.producersAccuracy().getInfo())\n",
    "# print('Kappa',originalErrorMatrix.kappa().getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of changes for each year\n",
    "\n",
    "for year in years[0:-1]:\n",
    "    year_list = ['dw_{}'.format(year),'dw_{}'.format(year+1)]\n",
    "    num_changes = pcf.calculateNumberOfChanges(gap_filled.select(year_list), year_list)\n",
    "\n",
    "    num_changes_mean = num_changes.reduceRegion(reducer=ee.Reducer.mean(), \n",
    "                                                  geometry=geometry,\n",
    "                                                  crs=crs, crsTransform=crsTransform, \n",
    "                                                  bestEffort=True, \n",
    "                                                  maxPixels=1e13, tileScale=4)\n",
    "    print('Number of changes from',year,'to',year+1,\"{:.4f}\".format(num_changes_mean.get('sum').getInfo()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply Temporal Filters\n",
    "\n",
    "<font size=\"4\">\n",
    "<br>\n",
    "Section 3.5.3. of the ATBD: Temporal filter:\n",
    "\"The temporal filter uses sequential classifications in a three-to-five-years unidirectional moving window to identify temporally non-permitted transitions. Based on generic rules (GR), the temporal filter inspects the central position of three to five consecutive years, and if the extremities of the consecutive years are identical but the centre position is not, then the central pixels are reclassified to match its temporal neighbour class. For the three years based temporal filter, a single central position shall exist, for the four and five years filters, two and there central positions are respectively considered.\n",
    "Another generic temporal rule is applied to extremity of consecutive years. In this case, a three consecutive years window is used and if the classifications of the first and last years are different from its neighbours, this values are replaced by the classification of its matching neighbours.\"\n",
    "    \n",
    "All code for the Temporal Filters was provided by the [Pampa Team](https://github.com/mapbiomas-brazil/pampa) in [this file](https://github.com/mapbiomas-brazil/pampa/blob/master/Step006_Filter_03_temporal.js)\n",
    "\n",
    "Functions were rewritten in Python and made independent of the land cover classification image.\n",
    "\n",
    "The MapBiomas implementation of the temporal filters includes the ability to perform the temporal filtering for one land cover class at a time.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load classifications into an image that will be filtered\n",
    "temporally_filtered = dynamic_world_classifications_image\n",
    "\n",
    "#Get a list of land cover values to apply the filters\n",
    "class_dictionary = dw_classes_dict.getInfo()\n",
    "order_of_values = [class_dictionary.get('trees'),class_dictionary.get('crops'),class_dictionary.get('built_area'),\n",
    "                  class_dictionary.get('grass'),class_dictionary.get('scrub'),class_dictionary.get('bare_ground'),\n",
    "                   class_dictionary.get('flooded_vegetation'),class_dictionary.get('water'),class_dictionary.get('snow_and_ice')]\n",
    "\n",
    "#Loop through order_of_values and apply temporal filters, in order applied by MapBiomas in https://github.com/mapbiomas-brazil/pampa/blob/master/Step006_Filter_03_temporal.js\n",
    "#We'll first apply the filter to the first year\n",
    "#Then apply the filter for the final year\n",
    "#Then apply the 3 year window, 4 year window, and 5 year window\n",
    "\n",
    "for i in np.arange(len(order_of_values)):\n",
    "    id_class = order_of_values[i] \n",
    "    temporally_filtered = pcf.applyMask3first(temporally_filtered, id_class, dw_band_names)\n",
    "\n",
    "for i in np.arange(len(order_of_values)):\n",
    "    id_class = order_of_values[i] \n",
    "    temporally_filtered = pcf.applyMask3last(temporally_filtered, id_class, dw_band_names)\n",
    "\n",
    "for i in np.arange(len(order_of_values)):\n",
    "    id_class = order_of_values[i] \n",
    "    temporally_filtered = pcf.applyWindow3years(temporally_filtered, id_class, dw_band_names)\n",
    "\n",
    "for i in np.arange(len(order_of_values)):\n",
    "    id_class = order_of_values[i] \n",
    "    temporally_filtered = pcf.applyWindow4years(temporally_filtered, id_class, dw_band_names)\n",
    "    \n",
    "for i in np.arange(len(order_of_values)):\n",
    "    id_class = order_of_values[i] \n",
    "    temporally_filtered = pcf.applyWindow5years(temporally_filtered, id_class, dw_band_names)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map before and after along with a layer to see pixels that changed\n",
    "changed_with_temporal_filter = temporally_filtered.select('dw_2017').neq(dynamic_world_classifications_image.select('dw_2017'))\n",
    "\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map3 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map3.addLayer(dynamic_world_classifications_image.select('dw_2016'),statesViz,name='2016 LC')\n",
    "Map3.addLayer(dynamic_world_classifications_image.select('dw_2017'),statesViz,name='2017 LC')\n",
    "Map3.addLayer(dynamic_world_classifications_image.select('dw_2018'),statesViz,name='2018 LC')\n",
    "Map3.addLayer(dynamic_world_classifications_image.select('dw_2019'),statesViz,name='2018 LC')\n",
    "Map3.addLayer(temporally_filtered.select('dw_2016'),statesViz,name='2016 Post Filter')\n",
    "Map3.addLayer(temporally_filtered.select('dw_2017'),statesViz,name='2017 Post Filter')\n",
    "Map3.addLayer(temporally_filtered.select('dw_2018'),statesViz,name='2018 Post Filter')\n",
    "Map3.addLayer(temporally_filtered.select('dw_2019'),statesViz,name='2019 Post Filter')\n",
    "Map3.addLayer(changed_with_temporal_filter,oneChangeDetectionViz,name='LC Classes in 2017 that changed after filter')\n",
    "#Grey areas show no change with the filter, red areas show change with the filter\n",
    "display(Map3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Calculate accuracy and confusion matrix for temporally filtered classifications on label data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label points\n",
    "labelPointsFC = ee.FeatureCollection(labelPoints_assetID.format('goldsboro'))\n",
    "\n",
    "#Save 2019 post-filtered DW classifications and rename to \"dw_filterd_classifications\"\n",
    "classifications_filtered_2019 = temporally_filtered.select('dw_2019').rename('dw_temp_filt_classifications')\n",
    "\n",
    "#Sample the 2019 classifications at each label point\n",
    "labelPointsWithFilteredDW = classifications_filtered_2019.sampleRegions(collection=labelPointsFC, \n",
    "                                                                        projection = projection_ee, \n",
    "                                                                        tileScale=4, geometries=True)\n",
    "\n",
    "#Calculate confusion matrix, which we will use for an accuracy assessment\n",
    "filteredErrorMatrix = labelPointsWithFilteredDW.errorMatrix('labels', 'dw_temp_filt_classifications')\n",
    "\n",
    "#Print the confusion matrix with the class names as a dataframe\n",
    "errorMatrixDf = gclass.pretty_print_confusion_matrix_multiclass(filteredErrorMatrix, full_dw_classes_str)\n",
    "#Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.\n",
    "print('Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.')\n",
    "display(errorMatrixDf)\n",
    "\n",
    "#You can also print further accuracy scores from the confusion matrix, however each one takes a couple minutes \n",
    "#to load\n",
    "print('Accuracy',filteredErrorMatrix.accuracy().getInfo())\n",
    "# print('Consumers Accuracy',originalErrorMatrix.consumersAccuracy().getInfo())\n",
    "# print('Producers Accuracy',originalErrorMatrix.producersAccuracy().getInfo())\n",
    "# print('Kappa',originalErrorMatrix.kappa().getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of changes for each year\n",
    "\n",
    "for year in years[0:-1]:\n",
    "    year_list = ['dw_{}'.format(year),'dw_{}'.format(year+1)]\n",
    "    num_changes = pcf.calculateNumberOfChanges(temporally_filtered.select(year_list), year_list)\n",
    "\n",
    "    num_changes_mean = num_changes.reduceRegion(reducer=ee.Reducer.mean(), \n",
    "                                                  geometry=geometry,\n",
    "                                                  crs=crs, crsTransform=crsTransform, \n",
    "                                                  bestEffort=True, \n",
    "                                                  maxPixels=1e13, tileScale=4)\n",
    "    print('Number of changes from',year,'to',year+1,\"{:.4f}\".format(num_changes_mean.get('sum').getInfo()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Spatial Filters\n",
    "<font size=\"4\">\n",
    "<br>\n",
    "Section 3.5.2. of the ATBD: Spatial filter:\n",
    "Spatial filter was applied to avoid unwanted modifications to the edges of the pixel groups (blobs), a spatial filter was built based on the “connectedPixelCount” function. Native to the GEE platform, this function locates connected components (neighbours) that share the same pixel value. Thus, only pixels that do not share connections to a predefined number of identical neighbours are considered isolated. In this filter, at least five connected pixels are needed to reach the minimum connection value. Consequently, the minimum mapping unit is directly affected by the spatial filter applied, and it was defined as 5 pixels (~0.5 ha).\n",
    "    \n",
    "All code for the spatial filter was provided within the [intregration-toolkit](https://github.com/mapbiomas-brazil/integration-toolkit) that is used to combine land cover classifications from each biome and cross-cutting theme team. The direct code was provided in [this file](https://github.com/mapbiomas-brazil/integration-toolkit/blob/master/mapbiomas-integration-toolkit.js).\n",
    "    \n",
    "Functions were rewritten in Python and made independent of the land cover classification image.\n",
    "\n",
    "The spatial filters are applied for each land cover class defined by the user. For each class the user can define the minimum number of connected pixels needed to not filter out the cluster. If the number of connected pixels is too small, the central pixel is replaced by the mode of the time series.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of dictionaries, where each dictionary contains 'classValue' representing the value of the land cover\n",
    "#class and 'minSize' representing the minimum connectedPixelCount needed to not be replaced by the filter\n",
    "\n",
    "# no_data=0\n",
    "# water=1\n",
    "# trees=2\n",
    "# grass=3\n",
    "# flooded_vegetation=4\n",
    "# crops=5\n",
    "# scrub_shrub=6\n",
    "# builtup=7\n",
    "# bare_ground=8\n",
    "# snow_ice=9\n",
    "# clouds=10\n",
    "\n",
    "filterParams = [\n",
    "    {'classValue': 1, 'minSize': 5},\n",
    "    {'classValue': 2, 'minSize': 5},\n",
    "    {'classValue': 3, 'minSize': 5},\n",
    "    {'classValue': 4, 'minSize': 5},\n",
    "    {'classValue': 5, 'minSize': 10},\n",
    "    {'classValue': 6, 'minSize': 5},\n",
    "    {'classValue': 7, 'minSize': 3},\n",
    "    {'classValue': 8, 'minSize': 5},\n",
    "    {'classValue': 9, 'minSize': 10},\n",
    "]\n",
    "\n",
    "#Load classifications into an image that will be filtered\n",
    "spatially_filtered = dynamic_world_classifications_image\n",
    "\n",
    "#Define empty list to append outputted images from spatial filter\n",
    "spatial_filter_output = []\n",
    "#Loop through years\n",
    "for band in dw_band_names:\n",
    "    #Apply spatial filter for one year using the filterParams\n",
    "    out_image = pcf.applySpatialFilter(spatially_filtered.select(band), filterParams)\n",
    "    #Append result to list\n",
    "    spatial_filter_output.append(out_image)\n",
    "#Convert list to image collection, then to multiband image\n",
    "spatially_filtered = ee.ImageCollection(spatial_filter_output).toBands().rename(dw_band_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the before and after!\n",
    "#The spatial filter depends on the scale, so to see the final results, reproject the image to the original 10 m resolution\n",
    "changed_with_spatial_filter = dynamic_world_classifications_image.select('dw_2017').neq(spatially_filtered.select('dw_2017').reproject(crs='EPSG:3857', scale=10))\n",
    "\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map4 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map4.addLayer(dynamic_world_classifications_image.select('dw_2017'),statesViz,name='2017 LC')\n",
    "Map4.addLayer(spatially_filtered.select('dw_2017').reproject(crs='EPSG:3857', scale=10),statesViz,name='2017 LC Post Spatial Filter')\n",
    "Map4.addLayer(changed_with_spatial_filter,oneChangeDetectionViz,name='Changed with spatial filter')\n",
    "#Grey areas show no change with the filter, red areas show change with the filter\n",
    "display(Map4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Calculate accuracy and confusion matrix for spatially filtered classifications on label data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label points\n",
    "labelPointsFC = ee.FeatureCollection(labelPoints_assetID.format('goldsboro'))\n",
    "\n",
    "#Save 2019 post-filtered DW classifications and rename to \"dw_filterd_classifications\"\n",
    "classifications_filtered_2019 = spatially_filtered.select('dw_2019').rename('dw_temp_spatial_filt_classifications')\n",
    "\n",
    "#Sample the 2019 classifications at each label point\n",
    "labelPointsWithFilteredDW = classifications_filtered_2019.sampleRegions(collection=labelPointsFC, \n",
    "                                                                        projection = projection_ee, \n",
    "                                                                        tileScale=4, geometries=True)\n",
    "\n",
    "#Calculate confusion matrix, which we will use for an accuracy assessment\n",
    "filteredErrorMatrix = labelPointsWithFilteredDW.errorMatrix('labels', 'dw_temp_spatial_filt_classifications')\n",
    "\n",
    "#Print the confusion matrix with the class names as a dataframe\n",
    "errorMatrixDf = gclass.pretty_print_confusion_matrix_multiclass(filteredErrorMatrix, full_dw_classes_str)\n",
    "#Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.\n",
    "print('Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.')\n",
    "display(errorMatrixDf)\n",
    "\n",
    "#You can also print further accuracy scores from the confusion matrix, however each one takes a couple minutes \n",
    "#to load\n",
    "print('Accuracy',filteredErrorMatrix.accuracy().getInfo())\n",
    "# print('Consumers Accuracy',originalErrorMatrix.consumersAccuracy().getInfo())\n",
    "# print('Producers Accuracy',originalErrorMatrix.producersAccuracy().getInfo())\n",
    "# print('Kappa',originalErrorMatrix.kappa().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Apply Incidence Filter\n",
    "<font size=\"4\">\n",
    "<br>\n",
    "Section 3.5.5. of the ATBD: Incident Filter\n",
    "\"An incident filter were applied to remove pixels that changed too many times in the 34 years of time spam. All pixels that changed more than eight times and is connected to less than 6 pixels was replaced by the MODE value of that given pixel position in the stack of years. This avoids changes in the border of the classes and helps to stabilize originally noise pixel trajectories. Each biome and cross-cutting themes may have constituted customized applications of incident filters, see more details in its respective appendices.\"\n",
    "\n",
    "This was not clearly implemented in the MapBiomas code, so this filter was coded by the WRI Team. The incidence filter finds all pixels that changed more than numChangesCutoff times and is connected to less than connectedPixelCutoff pixels, then replaces those pixels with the MODE value of that given pixel position in the stack of years.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load classifications into an image that will be filtered\n",
    "incident_filtered = dynamic_world_classifications_image\n",
    "\n",
    "#Calculate the number of changes in each pixel before the incidence filter\n",
    "num_changes = pcf.calculateNumberOfChanges(dynamic_world_classifications_image, dw_band_names)\n",
    "\n",
    "#Apply incidence filter\n",
    "incident_filtered = pcf.applyIncidenceFilter(incident_filtered, dw_band_names, dw_classes_dict, \n",
    "                                             numChangesCutoff = 2, connectedPixelCutoff=6)\n",
    "\n",
    "#Calculate the number of changes in each pixel before the incidence filter\n",
    "num_changes_post_incidence = pcf.calculateNumberOfChanges(incident_filtered, dw_band_names)\n",
    "\n",
    "#Calculate the difference in the number of changes before and after the filter\n",
    "changed_from_incidence = num_changes.neq(num_changes_post_incidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the results!\n",
    "numChangesViz = {'min': 0, 'max': 3, 'palette': ['131b7a','04ecff']}; #gray = 0, red = 1co\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map5 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map5.addLayer(num_changes,numChangesViz,name='Number of Changes Pre Filter')\n",
    "Map5.addLayer(num_changes_post_incidence,numChangesViz,name='Number of Changes Post Filter')\n",
    "Map5.addLayer(changed_from_incidence,oneChangeDetectionViz,name='Changed with Filter')\n",
    "display(Map5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Calculate accuracy and confusion matrix for incidence filtered classifications on label data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label points\n",
    "labelPointsFC = ee.FeatureCollection(labelPoints_assetID.format('goldsboro'))\n",
    "\n",
    "#Save 2019 post-filtered DW classifications and rename to \"dw_filterd_classifications\"\n",
    "classifications_filtered_2019 = incident_filtered.select('dw_2019').rename('dw_temp_incidence_filt_classifications')\n",
    "\n",
    "#Sample the 2019 classifications at each label point\n",
    "labelPointsWithFilteredDW = classifications_filtered_2019.sampleRegions(collection=labelPointsFC, \n",
    "                                                                        projection = projection_ee, \n",
    "                                                                        tileScale=4, geometries=True)\n",
    "\n",
    "#Calculate confusion matrix, which we will use for an accuracy assessment\n",
    "filteredErrorMatrix = labelPointsWithFilteredDW.errorMatrix('labels', 'dw_temp_incidence_filt_classifications')\n",
    "\n",
    "#Print the confusion matrix with the class names as a dataframe\n",
    "errorMatrixDf = gclass.pretty_print_confusion_matrix_multiclass(filteredErrorMatrix, full_dw_classes_str)\n",
    "#Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.\n",
    "print('Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.')\n",
    "display(errorMatrixDf)\n",
    "\n",
    "#You can also print further accuracy scores from the confusion matrix, however each one takes a couple minutes \n",
    "#to load\n",
    "print('Accuracy',filteredErrorMatrix.accuracy().getInfo())\n",
    "# print('Consumers Accuracy',originalErrorMatrix.consumersAccuracy().getInfo())\n",
    "# print('Producers Accuracy',originalErrorMatrix.producersAccuracy().getInfo())\n",
    "# print('Kappa',originalErrorMatrix.kappa().getInfo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply Frequency Filter\n",
    "<font size=\"4\">\n",
    "<br>\n",
    "Section 3.5.5. of the ATBD: Frequency Filter\n",
    "\"This filter takes into consideration the occurrence frequency throughout the entire time series. Thus, all class occurrence with less than given percentage of temporal persistence (eg. 3 years or fewer out of 33) are filtered out. This mechanism contributes to reducing the temporal oscillation associated to a given class, decreasing the number of false positives and preserving consolidated trajectories. Each biome and cross-cutting themes may have constituted customized applications of frequency filters, see more details in their respective appendices.\"\n",
    "\n",
    "This was not clearly implemented in the MapBiomas code, so this filter was coded by the WRI Team. All class occurrence with less than given percentage of temporal persistence (eg. 3 years or fewer out of 33) are replaced with the mode value of that given pixel position in the stack of years.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load classifications into an image that will be filtered\n",
    "frequency_filtered = dynamic_world_classifications_image\n",
    "\n",
    "#Define filterParams that defines the class name and the minimum number of occurances that need to occur\n",
    "filterParams = {'water':2, \n",
    "                'trees': 2, \n",
    "                'grass': 2,\n",
    "                'flooded_vegetation':2,\n",
    "                'crops': 2,\n",
    "                'scrub': 2,\n",
    "                'built_area': 2, \n",
    "                'bare_ground': 2, \n",
    "                'snow_and_ice': 2}\n",
    "filterParams = ee.Dictionary(filterParams)\n",
    "\n",
    "#Apply frequency filter\n",
    "frequency_filtered = pcf.applyFrequencyFilter(frequency_filtered, dw_band_names, \n",
    "                                              dw_classes_dict, filterParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get binary images of the land cover classifications for the current year\n",
    "binary_class_images = npv.convertClassificationsToBinaryImages(dynamic_world_classifications_image, dw_classes_dict)\n",
    "#Get the frequency of each class through the years by reducing the image collection to an image\n",
    "class_frequency = binary_class_images.reduce(ee.Reducer.sum().unweighted()).rename(filterParams.keys().getInfo())\n",
    "\n",
    "#Get binary images of the land cover classifications for the current year\n",
    "post_binary_class_images = npv.convertClassificationsToBinaryImages(frequency_filtered, dw_classes_dict)\n",
    "#Get the frequency of each class through the years by reducing the image collection to an image\n",
    "post_class_frequency = post_binary_class_images.reduce(ee.Reducer.sum().unweighted()).rename(filterParams.keys())\n",
    "\n",
    "changed_from_frequency_filter = class_frequency.neq(post_class_frequency)\n",
    "\n",
    "\n",
    "#Map the results!\n",
    "numChangesViz = {'min': 0, 'max': 3, 'palette': ['131b7a','04ecff']}; #gray = 0, red = 1co\n",
    "center = [35.410769, -78.100163]\n",
    "zoom = 12\n",
    "Map4 = geemap.Map(center=center, zoom=zoom,basemap=basemaps.Esri.WorldImagery,add_google_map = False)\n",
    "Map4.addLayer(class_frequency.select('grass'),numChangesViz,name='Number of Occurrences Pre Filter')\n",
    "Map4.addLayer(post_class_frequency.select('grass'),numChangesViz,name='Number of Occurrences Post Filter')\n",
    "Map4.addLayer(changed_from_frequency_filter.select('grass'),oneChangeDetectionViz,name='Changed with Filter')\n",
    "display(Map4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Calculate accuracy and confusion matrix for frequency filtered classifications on label data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load label points\n",
    "labelPointsFC = ee.FeatureCollection(labelPoints_assetID.format('goldsboro'))\n",
    "\n",
    "#Save 2019 post-filtered DW classifications and rename to \"dw_filterd_classifications\"\n",
    "classifications_filtered_2019 = frequency_filtered.select('dw_2019').rename('dw_temp_frequency_filt_classifications')\n",
    "\n",
    "#Sample the 2019 classifications at each label point\n",
    "labelPointsWithFilteredDW = classifications_filtered_2019.sampleRegions(collection=labelPointsFC, \n",
    "                                                                        projection = projection_ee, \n",
    "                                                                        tileScale=4, geometries=True)\n",
    "\n",
    "#Calculate confusion matrix, which we will use for an accuracy assessment\n",
    "filteredErrorMatrix = labelPointsWithFilteredDW.errorMatrix('labels', 'dw_temp_frequency_filt_classifications')\n",
    "\n",
    "#Print the confusion matrix with the class names as a dataframe\n",
    "errorMatrixDf = gclass.pretty_print_confusion_matrix_multiclass(filteredErrorMatrix, full_dw_classes_str)\n",
    "#Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.\n",
    "print('Axis 1 (the rows) of the matrix correspond to the actual values, and Axis 0 (the columns) to the predicted values.')\n",
    "display(errorMatrixDf)\n",
    "\n",
    "#You can also print further accuracy scores from the confusion matrix, however each one takes a couple minutes \n",
    "#to load\n",
    "print('Accuracy',filteredErrorMatrix.accuracy().getInfo())\n",
    "# print('Consumers Accuracy',originalErrorMatrix.consumersAccuracy().getInfo())\n",
    "# print('Producers Accuracy',originalErrorMatrix.producersAccuracy().getInfo())\n",
    "# print('Kappa',originalErrorMatrix.kappa().getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
